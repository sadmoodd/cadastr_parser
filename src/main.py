# -*- coding: utf-8 -*-
"""
main.py
----------------
Ğ“Ğ›ĞĞ’ĞĞ«Ğ™ Ğ¡ĞšĞ Ğ˜ĞŸĞ¢ Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Qwen3-VL Ñ‡ĞµÑ€ĞµĞ· Hugging Face Router.

Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ:
- Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğµ Ğ¼ĞµĞ½Ñ Ğ´Ğ»Ñ ÑƒĞ´Ğ¾Ğ±ÑÑ‚Ğ²Ğ°
- Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğº Qwen3-VL Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ OCR
- ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° PDF Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM
- Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Excel
- ĞšÑ€Ğ°Ñ‚ĞºĞ¸Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚ Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ
"""

import sys
import os
from pathlib import Path
from typing import Dict, List, Tuple
import time
import base64
import json
import requests
import re

from pdf2image import convert_from_path
from PIL import Image
from io import BytesIO

# Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ²ÑĞµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
from logger_cfg import setup_logger
from settings import INPUT_DIR, OUTPUT_DIR, LOGS_DIR, MESSAGES, REGEX_PATTERNS
from table_builder import (
    create_empty_dataframe,
    create_row_from_extracted_data,
    create_error_row,
    add_rows_batch,
    fill_numbers_column,
    get_dataframe_info,
)
from excel_writer import save_dataframe_to_excel, get_file_size

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

logger = setup_logger(__name__)

# Hugging Face Chat Completions API (Router)
HF_TOKEN = os.getenv("HF_TOKEN")
if not HF_TOKEN:
    raise RuntimeError("HF_TOKEN env var is not set")
HF_API_URL = "https://router.huggingface.co/v1/chat/completions"
MODEL_NAME = "Qwen/Qwen3-VL-30B-A3B-Instruct"

# System prompt Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
SYSTEM_PROMPT = """Ğ¢Ñ‹ - ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚ Ğ¿Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ•Ğ“Ğ Ğ (Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ³Ğ¾ÑÑƒĞ´Ğ°Ñ€ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€ĞµĞµÑÑ‚Ñ€ Ğ½ĞµĞ´Ğ²Ğ¸Ğ¶Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸).

Ğ¢Ğ²Ğ¾Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°: Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ/Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ•Ğ“Ğ Ğ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ JSON:

{
    "cadastral_number": "XX:XX:XXXXXXX:XXX Ğ¸Ğ»Ğ¸ null",
    "address": "ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ°Ğ´Ñ€ĞµÑ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¸Ğ»Ğ¸ null",
    "area": "ĞŸĞ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ Ğ² Ğ¼Â² Ğ¸Ğ»Ğ¸ null",
    "owner": "Ğ¤Ğ˜Ğ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¸ĞºĞ° Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ null",
    "permitted_use": "Ğ’Ğ¸Ğ´ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ null",
    "cadastral_cost": "ĞšĞ°Ğ´Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ÑƒĞ± Ğ¸Ğ»Ğ¸ null",
    "land_category": "ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ Ğ·ĞµĞ¼ĞµĞ»ÑŒ Ğ¸Ğ»Ğ¸ null",
    "rental_data": {
        "rent_type": "Ğ¢Ğ¸Ğ¿ Ğ¾Ğ±Ñ€ĞµĞ¼ĞµĞ½ĞµĞ½Ğ¸Ñ (ĞÑ€ĞµĞ½Ğ´Ğ°/Ğ¡ĞµÑ€Ğ²Ğ¸Ñ‚ÑƒÑ‚/Ğ¸ Ñ‚.Ğ´.) Ğ¸Ğ»Ğ¸ null",
        "period_start": "Ğ”Ğ°Ñ‚Ğ° Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ğ”Ğ”.ĞœĞœ.Ğ“Ğ“Ğ“Ğ“ Ğ¸Ğ»Ğ¸ null",
        "period_end": "Ğ”Ğ°Ñ‚Ğ° ĞºĞ¾Ğ½Ñ†Ğ° Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ğ”Ğ”.ĞœĞœ.Ğ“Ğ“Ğ“Ğ“ Ğ¸Ğ»Ğ¸ null",
        "tenant": "ĞĞ°Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ñ€ĞµĞ½Ğ´Ğ°Ñ‚Ğ¾Ñ€Ğ°/Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ null"
    }
}

ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ:
1. Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°Ğ¹ Ğ¢ĞĞ›Ğ¬ĞšĞ JSON, Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°
2. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ null Ğ´Ğ»Ñ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ğ¹ ĞºĞ»ÑÑ‡Ğ¸
3. Ğ§ĞµÑ‚ĞºĞ¾ ÑĞ»ĞµĞ´ÑƒĞ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ JSON
4. Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°Ğ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ²Ğ½Ğ¾ Ğ²Ğ¸Ğ´Ğ½Ğ° Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğµ
5. Ğ”Ğ»Ñ ĞºĞ°Ğ´Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ½Ğ¾Ğ¼ĞµÑ€Ğ° Ğ¸Ñ‰Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚: XX:XX:XXXXXXX:XXX
6. Ğ”Ğ»Ñ Ğ´Ğ°Ñ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚: Ğ”Ğ”.ĞœĞœ.Ğ“Ğ“Ğ“Ğ“
7. ĞĞµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞ¹ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾ÑÑĞ½ĞµĞ½Ğ¸Ñ

ĞÑ‚Ğ²ĞµÑ‚ Ğ”ĞĞ›Ğ–Ğ•Ğ Ğ±Ñ‹Ñ‚ÑŒ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğ¹ JSON!"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ’Ğ¡ĞŸĞĞœĞĞ“ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ• Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ˜ Ğ”Ğ›Ğ¯ PDF/Ğ˜Ğ—ĞĞ‘Ğ ĞĞ–Ğ•ĞĞ˜Ğ™
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def pdf_to_images(pdf_path: Path, dpi: int = 150) -> List[Image.Image]:
    """ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ PDF Ğ² ÑĞ¿Ğ¸ÑĞ¾Ğº PIL.Image."""
    logger.debug(f"ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ñ PDF Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: {pdf_path}")
    images = convert_from_path(str(pdf_path), dpi=dpi)
    logger.debug(f"Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¾ {len(images)} ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†")
    return images


def image_to_base64(image: Image.Image, max_size=(1024, 1024), quality=80) -> str:
    """ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ PIL.Image Ğ² base64 JPEG."""
    img = image.copy()
    img.thumbnail(max_size, Image.Resampling.LANCZOS)
    buf = BytesIO()
    img.save(buf, format="JPEG", quality=quality)
    return base64.b64encode(buf.getvalue()).decode("utf-8")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ˜ ĞœĞ•ĞĞ®
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def print_menu():
    """Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ³Ğ»Ğ°Ğ²Ğ½Ğ¾Ğµ Ğ¼ĞµĞ½Ñ."""
    print("\n" + "=" * 70)
    print("ğŸ›ï¸  PDF PARSER Ğ•Ğ“Ğ N - AI AGENT (Qwen3-VL)")
    print("=" * 70)
    print("\nğŸ“‹ ĞœĞ•ĞĞ®:\n")
    print("  1. ğŸš€ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ²ÑĞµ PDF Ğ¸Ğ· Ğ¿Ğ°Ğ¿ĞºĞ¸ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ")
    print("  2. ğŸ“ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ PDF Ğ¸Ğ· ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¿ĞºĞ¸")
    print("  3. ğŸ“Š ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚")
    print("  4. ğŸ§¹ ĞÑ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ½Ğ°Ñ‡Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ½Ğ¾Ğ²Ğ¾")
    print("  5. âŒ Ğ’Ñ‹Ñ…Ğ¾Ğ´\n")
    print("=" * 70)


def get_user_choice() -> int:
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ."""
    while True:
        try:
            choice = int(input("\nğŸ‘¤ Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ½Ğ¾Ğ¼ĞµÑ€ Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ¼ĞµĞ½Ñ (1-5): "))
            if 1 <= choice <= 5:
                return choice
            else:
                print("âŒ ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ²Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾ Ğ¾Ñ‚ 1 Ğ´Ğ¾ 5")
        except ValueError:
            print("âŒ ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ²Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾")


def get_custom_folder() -> Path:
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½ÑƒÑ Ğ¿Ğ°Ğ¿ĞºÑƒ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ."""
    while True:
        folder_path = input("\nğŸ“ Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¿Ğ°Ğ¿ĞºĞµ (Ğ¸Ğ»Ğ¸ Enter Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ): ").strip()
        if not folder_path:
            return INPUT_DIR

        path = Path(folder_path)
        if path.exists() and path.is_dir():
            return path
        else:
            print(f"âŒ ĞŸĞ°Ğ¿ĞºĞ° Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°: {folder_path}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞŸĞĞ¡Ğ¢-ĞĞ‘Ğ ĞĞ‘ĞĞ¢ĞšĞ Ğ”ĞĞĞĞ«Ğ¥ Ğ¡ ĞŸĞĞœĞĞ©Ğ¬Ğ® REGEX_PATTERNS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def normalize_with_patterns(data: Dict, patterns: Dict) -> Dict:
    """
    ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ REGEX_PATTERNS,
    Ğ½Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ JSON.
    """
    if not isinstance(data, dict):
        return data

    # ĞšĞ°Ğ´Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ¾Ğ¼ĞµÑ€
    cad_key = "cadastral_number"
    if cad_key in data and isinstance(data[cad_key], str) and data[cad_key]:
        pat = patterns.get("cadastral_number")
        if pat:
            m = re.search(pat, data[cad_key])
            if m:
                data[cad_key] = m.group(0)

    # ĞŸĞ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ
    area_key = "area"
    if area_key in data and isinstance(data[area_key], str) and data[area_key]:
        pat = patterns.get("area")
        if pat:
            m = re.search(pat, data[area_key])
            if m:
                data[area_key] = m.group(0)

    # ĞšĞ°Ğ´Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ
    cost_key = "cadastral_cost"
    if cost_key in data and isinstance(data[cost_key], str) and data[cost_key]:
        pat = patterns.get("cadastral_cost")
        if pat:
            m = re.search(pat, data[cost_key])
            if m:
                data[cost_key] = m.group(0)

    # Ğ”Ğ°Ñ‚Ñ‹ Ğ°Ñ€ĞµĞ½Ğ´Ñ‹
    rental = data.get("rental_data")
    if isinstance(rental, dict):
        date_pat = patterns.get("date")
        if date_pat:
            for key in ["period_start", "period_end"]:
                if key in rental and isinstance(rental[key], str) and rental[key]:
                    m = re.search(date_pat, rental[key])
                    if m:
                        rental[key] = m.group(0)

    return data

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ’Ğ—ĞĞ˜ĞœĞĞ”Ğ•Ğ™Ğ¡Ğ¢Ğ’Ğ˜Ğ• Ğ¡ QWEN3-VL Ğ§Ğ•Ğ Ğ•Ğ— HF ROUTER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def query_deepseek_ocr(pdf_path: Path) -> Dict:
    """
    Ğ—Ğ°Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°ĞµÑ‚ Qwen3-VL Ñ‡ĞµÑ€ĞµĞ· Hugging Face Router Ğ´Ğ»Ñ Ğ•Ğ“Ğ Ğ PDF.
    Ğ¡ĞµĞ¹Ñ‡Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° PDF.
    """
    try:
        logger.debug(f"Ğ—Ğ°Ğ¿Ñ€Ğ¾Ñ Ğº Qwen3-VL Ğ´Ğ»Ñ {pdf_path.name}")

        # 1) PDF -> image
        images = pdf_to_images(pdf_path, dpi=300)
        if not images:
            logger.error(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ PDF Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: {pdf_path}")
            return None

        first_page = images[0]
        image_b64 = image_to_base64(first_page)
        image_data_url = f"data:image/jpeg;base64,{image_b64}"

        # 2) ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° messages
        messages = [
            {
                "role": "system",
                "content": SYSTEM_PROMPT,
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": (
                            "ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ ÑÑ‚Ğ¾Ñ‚ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚ Ğ²Ñ‹Ğ¿Ğ¸ÑĞºĞ¸ Ğ•Ğ“Ğ Ğ Ğ¸ Ğ²ĞµÑ€Ğ½Ğ¸ Ğ¢ĞĞ›Ğ¬ĞšĞ JSON "
                            "Ğ² ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ² system-Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ."
                        ),
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": image_data_url
                        },
                    },
                ],
            },
        ]

        headers = {
            "Authorization": f"Bearer {HF_TOKEN}",
            "Content-Type": "application/json",
        }

        payload = {
            "model": MODEL_NAME,
            "messages": messages,
            "max_tokens": 4096,
            "temperature": 0.1,
            "response_format": {
                "type": "json_object"
            },
        }

        response = requests.post(
            HF_API_URL,
            headers=headers,
            json=payload,
            timeout=120,
        )

        if response.status_code != 200:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° API: {response.status_code}")
            logger.error(f"ĞÑ‚Ğ²ĞµÑ‚: {response.text}")
            return None

        data = response.json()

        try:
            result_text = data["choices"][0]["message"]["content"]
        except (KeyError, IndexError, TypeError) as e:
            logger.error(f"ĞĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°: {e}; data={data}")
            return None

        logger.debug(f"Ğ¡Ñ‹Ñ€Ğ¾Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {result_text[:300]}")

        try:
            parsed = json.loads(result_text)
        except json.JSONDecodeError as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° JSON Ğ¸Ğ· Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {e}")
            return None

        # ĞŸĞ¾ÑÑ‚-Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ REGEX_PATTERNS
        try:
            normalized = normalize_with_patterns(parsed, REGEX_PATTERNS)
        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ REGEX_PATTERNS: {e}")
            normalized = parsed

        return normalized

    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ Ğº Qwen3-VL: {e}")
        return None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞĞ‘Ğ ĞĞ‘ĞĞ¢ĞšĞ ĞĞ”ĞĞĞ“Ğ Ğ˜ ĞĞ•Ğ¡ĞšĞĞ›Ğ¬ĞšĞ˜Ğ¥ PDF
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def process_single_pdf_with_ai(
    pdf_path: Path,
    row_number: int
) -> Tuple[bool, Dict]:
    """
    ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ´Ğ¸Ğ½ PDF Ñ„Ğ°Ğ¹Ğ» Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Qwen3-VL.
    """
    pdf_name = pdf_path.name

    try:
        logger.info(f"[{row_number}] ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°: {pdf_name}")

        data = query_deepseek_ocr(pdf_path)

        if not data:
            logger.warning(f"ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ²ĞµÑ€Ğ½ÑƒĞ»Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ {pdf_name}")
            error_row = create_error_row(pdf_name, "AI API Ğ½Ğµ Ğ²ĞµÑ€Ğ½ÑƒĞ» Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ", row_number)
            return False, error_row

        row = create_row_from_extracted_data(data, pdf_name, row_number)

        logger.info(f"âœ“ {pdf_name}")
        return True, row

    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ {pdf_name}: {e}")
        error_row = create_error_row(pdf_name, str(e), row_number)
        return False, error_row


def find_pdf_files(search_path: Path) -> List[Path]:
    """Ğ˜Ñ‰ĞµÑ‚ Ğ²ÑĞµ PDF Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ² Ğ¿Ğ°Ğ¿ĞºĞµ."""
    if not search_path.exists():
        logger.error(f"ĞŸĞ°Ğ¿ĞºĞ° Ğ½Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚: {search_path}")
        return []

    pdf_files = list(search_path.glob("*.pdf")) + list(search_path.glob("*.PDF"))

    if not pdf_files:
        logger.warning(f"ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ PDF Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ² {search_path}")
        return []

    return sorted(pdf_files)


def process_all_pdfs_ai(pdf_files: List[Path]) -> Dict:
    """ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²ÑĞµ PDF Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Qwen3-VL AI."""
    logger.info(f"ĞĞ°Ñ‡Ğ°Ğ»Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ {len(pdf_files)} PDF Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²")

    stats = {
        'total_files': len(pdf_files),
        'successful': 0,
        'failed': 0,
        'rows': [],
        'processing_time': 0,
    }

    start_time = time.time()

    for idx, pdf_file in enumerate(pdf_files, 1):
        success, row = process_single_pdf_with_ai(pdf_file, idx)

        stats['rows'].append(row)

        if success:
            stats['successful'] += 1
        else:
            stats['failed'] += 1

        status_symbol = "âœ“" if success else "âœ—"
        print(f"[{idx}/{len(pdf_files)}] {status_symbol} {pdf_file.name}")

    stats['processing_time'] = time.time() - start_time

    return stats

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ ĞĞ‘ĞĞ¢Ğ Ğ¡ DATAFRAME/EXCEL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_final_dataframe(rows: List[Dict]):
    """Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ DataFrame."""
    df = create_empty_dataframe()

    if rows:
        df = add_rows_batch(df, rows)

    df = fill_numbers_column(df)

    return df


def print_brief_report(stats: Dict, output_file: str = None):
    """Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚."""
    print(f"\n{'='*70}")
    print(f"âœ… ĞĞ‘Ğ ĞĞ‘ĞĞ¢ĞšĞ Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•ĞĞ")
    print(f"{'='*70}")
    print(f"ğŸ“Š Ğ’ÑĞµĞ³Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²: {stats['total_files']}")
    print(f"âœ“  Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾: {stats['successful']}")
    print(f"âœ—  ĞÑˆĞ¸Ğ±Ğ¾Ğº: {stats['failed']}")
    print(f"â±ï¸  Ğ’Ñ€ĞµĞ¼Ñ: {stats['processing_time']:.2f} ÑĞµĞº")

    if output_file:
        print(f"ğŸ“ Excel Ñ„Ğ°Ğ¹Ğ»: {output_file}")
        print(f"   Ğ Ğ°Ğ·Ğ¼ĞµÑ€: {get_file_size(output_file)}")

    print(f"{'='*70}\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞœĞ•ĞĞ®: ĞĞ‘Ğ ĞĞ‘ĞĞ¢ĞšĞ, ĞŸĞĞšĞĞ— Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢ĞĞ’, ĞĞ§Ğ˜Ğ¡Ğ¢ĞšĞ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def process_pdfs_menu():
    """ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ PDF Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿Ğ°Ğ¿ĞºĞ¸."""
    print("\nğŸ“ Ğ’Ğ«Ğ‘ĞĞ  ĞŸĞĞŸĞšĞ˜")
    print("=" * 70)

    use_default = input("Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ğ¿ĞºÑƒ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ? (Ğ´Ğ°/Ğ½ĞµÑ‚): ").lower().strip()

    if use_default in ['Ğ´Ğ°', 'yes', 'y', '']:
        search_path = INPUT_DIR
        print(f"ğŸ“ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¿Ğ°Ğ¿ĞºĞ°: {search_path}")
    else:
        search_path = get_custom_folder()
        print(f"ğŸ“ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¿Ğ°Ğ¿ĞºĞ°: {search_path}")

    pdf_files = find_pdf_files(search_path)

    if not pdf_files:
        print(f"âŒ ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ PDF Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ² Ğ¿Ğ°Ğ¿ĞºĞµ: {search_path}")
        return

    print(f"\nâœ… ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(pdf_files)} PDF Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²:")
    for i, pdf in enumerate(pdf_files, 1):
        print(f"   {i}. {pdf.name}")

    confirm = input("\nĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ? (Ğ´Ğ°/Ğ½ĞµÑ‚): ").lower().strip()
    if confirm not in ['Ğ´Ğ°', 'yes', 'y', '']:
        print("âŒ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¾Ñ‚Ğ¼ĞµĞ½ĞµĞ½Ğ°")
        return

    print(f"\nâ³ ĞĞ‘Ğ ĞĞ‘ĞĞ¢ĞšĞ Ğ’ ĞŸĞ ĞĞ¦Ğ•Ğ¡Ğ¡Ğ•...")
    print("=" * 70 + "\n")

    stats = process_all_pdfs_ai(pdf_files)

    print(f"\nğŸ“‹ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹...")
    df = create_final_dataframe(stats['rows'])

    info = get_dataframe_info(df)
    print(f"âœ“ Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°: {info['total_rows']} ÑÑ‚Ñ€Ğ¾Ğº Ã— {info['total_columns']} ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº")

    print(f"ğŸ“Š Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Excel...")
    output_file = save_dataframe_to_excel(df)

    if output_file:
        print(f"âœ“ {output_file}")

    print_brief_report(stats, output_file)

    logger.info(f"ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°: ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ {stats['successful']}/{stats['total_files']}")

def show_last_result():
    """ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚."""
    excel_file = OUTPUT_DIR / "output_cadastre_data.xlsx"

    if not excel_file.exists():
        print("\nâŒ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ„Ğ°Ğ¹Ğ» Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
        return

    import pandas as pd

    df = pd.read_excel(excel_file)

    print(f"\nğŸ“Š ĞŸĞĞ¡Ğ›Ğ•Ğ”ĞĞ˜Ğ™ Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢")
    print("=" * 70)
    print(f"Ğ¤Ğ°Ğ¹Ğ»: {excel_file}")
    print(f"Ğ Ğ°Ğ·Ğ¼ĞµÑ€: {get_file_size(str(excel_file))}")
    print(f"Ğ¡Ñ‚Ñ€Ğ¾Ğº: {len(df)}")
    print(f"ĞšĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº: {len(df.columns)}")
    print(f"\n{df.head(5).to_string()}")
    print("=" * 70 + "\n")

def clear_data():
    """ĞÑ‡Ğ¸Ñ‰Ğ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ."""
    confirm = input("\nâš ï¸  Ğ’Ñ‹ ÑƒĞ²ĞµÑ€ĞµĞ½Ñ‹? Ğ­Ñ‚Ğ¾ ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Excel Ñ„Ğ°Ğ¹Ğ». (Ğ´Ğ°/Ğ½ĞµÑ‚): ").lower().strip()

    if confirm in ['Ğ´Ğ°', 'yes', 'y']:
        excel_file = OUTPUT_DIR / "output_cadastre_data.xlsx"
        if excel_file.exists():
            excel_file.unlink()
            print("âœ“ Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ñ‹")
        else:
            print("â„¹ï¸  ĞĞµÑ‡ĞµĞ³Ğ¾ Ğ¾Ñ‡Ğ¸Ñ‰Ğ°Ñ‚ÑŒ")
    else:
        print("âŒ ĞÑ‚Ğ¼ĞµĞ½Ğ¾")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    """Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ."""

    while True:
        print_menu()
        choice = get_user_choice()

        if choice == 1:
            search_path = INPUT_DIR
            pdf_files = find_pdf_files(search_path)

            if not pdf_files:
                print(f"\nâŒ ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ PDF Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ²: {search_path}")
                continue

            print(f"\nâœ… ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(pdf_files)} Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²")
            print("â³ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ...\n")

            stats = process_all_pdfs_ai(pdf_files)

            print(f"\nğŸ“‹ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹...")
            df = create_final_dataframe(stats['rows'])

            info = get_dataframe_info(df)
            print(f"âœ“ {info['total_rows']} ÑÑ‚Ñ€Ğ¾Ğº")

            print(f"ğŸ“Š Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Excel...")
            output_file = save_dataframe_to_excel(df)

            print_brief_report(stats, output_file)

        elif choice == 2:
            process_pdfs_menu()

        elif choice == 3:
            show_last_result()

        elif choice == 4:
            clear_data()

        elif choice == 5:
            print("\nğŸ‘‹ Ğ”Ğ¾ ÑĞ²Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ!\n")
            sys.exit(0)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¿Ñ€ĞµÑ€Ğ²Ğ°Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼")
        sys.exit(0)
    except Exception as e:
        logger.error(f"ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {e}")
        print(f"\nâŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}")
        sys.exit(11)
